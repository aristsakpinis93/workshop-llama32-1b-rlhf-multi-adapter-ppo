{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e497d314-b456-4261-a7ad-3e9040e010af",
   "metadata": {},
   "source": [
    "# Prep work\n",
    "\n",
    "Before starting the training, we need to create model repositories on the HuggingFace model hub for both our reward model adapters and our final RLHF model adapters.\n",
    "\n",
    "Also, in oder to be able to use the Llama 3.1 8b instruct model we need to accept the license terms of the model in the HuggingFace model hub. \n",
    "\n",
    "To authenticate against the HuggingFace model hub we need to create an access token, which we will use later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260e672-e033-4470-8683-cedeca74ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model to be fine-tuned\n",
    "#model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0574f-632e-4897-af95-ead0144bae8a",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e592547-c863-45e8-afce-d0a613e6f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "%pip install -U torch==2.2.0+cu118 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1114a-caf0-4625-a90a-9b120b4a393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies from requirements.txt file\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4605a5-553e-409b-b898-9d10b08a7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import botocore\n",
    "import bitsandbytes as bnb\n",
    "import multiprocessing\n",
    "import sys\n",
    "import functools\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
    "from trl import ModelConfig, RewardConfig, PPOConfig, PPOTrainer, RewardTrainer, AutoModelForCausalLMWithValueHead, get_kbit_device_map, get_peft_config, get_quantization_config\n",
    "from trl.core import LengthSampler\n",
    "from accelerate import Accelerator\n",
    "from peft import AutoPeftModelForCausalLM, AutoPeftModelForSequenceClassification, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from sagemaker.remote_function import remote\n",
    "from tqdm import tqdm\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a544fb-a90f-48f7-a49e-5f41496a1d65",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Creation of S3 bucket for data and model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1adfa-58ad-4c14-80e9-95522c313cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "def create_s3_bucket_for_models(bucket_name=None, region=None):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket for storing SageMaker models.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Optional name for the S3 bucket. If not provided, a name will be generated.\n",
    "        region: AWS region to create the bucket in. If not provided, uses the SageMaker session's region.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (bucket_name, s3_output_path) - The name of the created bucket and the S3 path for model output\n",
    "    \"\"\"\n",
    "    # Initialize boto3 clients\n",
    "    s3_client = boto3.client('s3')\n",
    "    session = Session()\n",
    "    \n",
    "    # Get the AWS region if not provided\n",
    "    if not region:\n",
    "        region = session.boto_region_name\n",
    "    \n",
    "    # Generate a unique bucket name if not provided\n",
    "    if not bucket_name:\n",
    "        timestamp = int(time.time())\n",
    "        random_suffix = str(uuid.uuid4())[:8]\n",
    "        bucket_name = f\"sagemaker-model-artifacts-{timestamp}-{random_suffix}\"\n",
    "    \n",
    "    # Create the S3 bucket\n",
    "    try:\n",
    "        if region == 'us-east-1':\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration=location\n",
    "            )\n",
    "        print(f\"Created S3 bucket: {bucket_name}\")\n",
    "        \n",
    "        # Create a folder for model output\n",
    "        s3_output_path = f\"s3://{bucket_name}/models\"\n",
    "        s3_data_path = f\"s3://{bucket_name}/data\"\n",
    "        \n",
    "        return bucket_name, s3_output_path, s3_data_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating S3 bucket: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Create a bucket for your SageMaker models\n",
    "bucket_name, s3_output_path, s3_data_path = create_s3_bucket_for_models()\n",
    "print(f\"Model output path: {s3_output_path}, Data output path: {s3_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f95925-216b-4cc5-bdfa-7f28c50e56ea",
   "metadata": {},
   "source": [
    "# Data preperation\n",
    "\n",
    "## Reward model training dataset\n",
    "\n",
    "Dataset used: Anthropic HH-RLHF (helpful) - https://huggingface.co/datasets/Anthropic/hh-rlhf\n",
    "\n",
    "Target format: \n",
    "```json\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
    "        num_rows: _\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
    "        num_rows: _\n",
    "    })\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0b54c5-6e15-40d1-9dbc-8bcf75f2d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to huggingface\n",
    "hf_token = \"***HF_TOKEN***\"\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77f9a6-6a3d-4693-97dd-95864c1de3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ds = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"helpful-base\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175b469-eb3d-47f6-b2cc-f7ecf2eb8909",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b3887-4558-4f52-9031-a0438f824077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dialogue(input_text):\n",
    "    # Split the input by lines and initialize variables\n",
    "    lines = input_text.strip().split(\"\\n\\n\")\n",
    "    dialogue_list = []\n",
    "\n",
    "    # Iterate through each line and extract the dialogue\n",
    "    for line in lines:\n",
    "        # Check if the line starts with \"Human\" or \"Assistant\" and split accordingly\n",
    "        if line.startswith(\"Human:\"):\n",
    "            role = \"user\"\n",
    "            content = line.replace(\"Human: \", \"\").strip()\n",
    "        elif line.startswith(\"Assistant:\"):\n",
    "            role = \"assistant\"\n",
    "            content = line.replace(\"Assistant: \", \"\").strip()\n",
    "        else:\n",
    "            # If the line doesn't start with \"Human\" or \"Assistant\", it's part of the previous message's content\n",
    "            # Append it to the last message's content\n",
    "            dialogue_list[-1][\"content\"] += \"\\n\\n\" + line.strip()\n",
    "            continue\n",
    "\n",
    "        # Append the extracted dialogue piece to the list\n",
    "        dialogue_list.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    return dialogue_list\n",
    "\n",
    "def process(row):\n",
    "        row[\"chosen\"] = extract_dialogue(row[\"chosen\"])\n",
    "        row[\"rejected\"] = extract_dialogue(row[\"rejected\"])\n",
    "        row[\"prompt\"] = row[\"chosen\"][0][\"content\"]\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec141d63-be1f-455f-b0ad-8bdd0f4d2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_processed = ds.map(\n",
    "        process,\n",
    "        load_from_cache_file=False,\n",
    "    )\n",
    "ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a51ee4-c025-4e3c-a8e4-c35b261d4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_processed['train'][67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9bb8e-5dc3-4089-b1eb-4a100629228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting to llama prompt template format: https://github.com/meta-llama/llama-recipes\n",
    "system_prompt = \"Please answer the user's question to the best of your knowledge. If you don't know the answer respond that you don't know.\"\n",
    "\n",
    "def encode_dialogue_turn(message):\n",
    "    return f'<|start_header_id|>{message.get(\"role\")}<|end_header_id|>{message.get(\"content\")}<|eot_id|>'\n",
    "\n",
    "def encode_dialogue(dialogue):\n",
    "    if system_prompt:\n",
    "        return f'<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|>{functools.reduce(lambda a, b: a + encode_dialogue_turn(b), dialogue, \"\")}'\n",
    "    else:\n",
    "        return f'<|begin_of_text|>{functools.reduce(lambda a, b: a + encode_dialogue_turn(b), dialogue, \"\")}'\n",
    "\n",
    "\n",
    "def encode_row(item):\n",
    "    return {\"chosen\": encode_dialogue(item[\"chosen\"]), \"rejected\": encode_dialogue(item[\"rejected\"]), \"prompt\": item[\"prompt\"]}\n",
    "                                      \n",
    "def encode_dataset(dataset):\n",
    "    return list(map(encode_row, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9822d-5fea-4a70-b04e-e532d8601128",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = ds_processed.map(encode_row)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b6a823-e0cd-480b-b2c1-2175af34ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset['train'][67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40caca75-7b73-40db-a428-9b4a1d539981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff8d6eb-20b9-428a-9826-9cb57837bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and stack into target format\n",
    "def preprocess_function(examples):\n",
    "    new_examples = {\n",
    "        \"input_ids_chosen\": [],\n",
    "        \"attention_mask_chosen\": [],\n",
    "        \"input_ids_rejected\": [],\n",
    "        \"attention_mask_rejected\": [],\n",
    "    }\n",
    "    for chosen, rejected in zip(examples[\"chosen\"], examples[\"rejected\"]):\n",
    "        tokenized_chosen = tokenizer(chosen)\n",
    "        tokenized_rejected = tokenizer(rejected)\n",
    "\n",
    "        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
    "        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
    "\n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4661c7-d0bf-4763-afda-5554c44ee250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_dataset_hhrlhf = encoded_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "    ).remove_columns([\"chosen\", \"rejected\", \"prompt\"])\n",
    "tokenized_dataset_hhrlhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6061c2-a93f-4a67-aa0b-e6ba0ccfb8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "#from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# Define the S3 path\n",
    "s3_bucket = bucket_name\n",
    "dataset_path_hhrlhf = f's3://{s3_bucket}/experiments-hhrlhf/helpful-base-train-test-tokenized-llama318binstruct'\n",
    "\n",
    "# Verify S3 bucket permissions first\n",
    "s3_client = boto3.client('s3')\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=s3_bucket)\n",
    "    print(f\"Successfully connected to bucket: {s3_bucket}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to bucket: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Save the dataset to S3 using the appropriate filesystem\n",
    "try:\n",
    "    # Make sure you have the s3fs package installed\n",
    "    tokenized_dataset_hhrlhf.save_to_disk(\n",
    "        dataset_path_hhrlhf, \n",
    "        fs=S3FileSystem()\n",
    "    )\n",
    "    print(f\"Successfully uploaded dataset to: {dataset_path_hhrlhf}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading to S3: {e}\")\n",
    "    \n",
    "    # Alternative approach if the above fails\n",
    "    print(\"Trying alternative approach...\")\n",
    "    #fs = s3fs.S3FileSystem()\n",
    "    tokenized_dataset_hhrlhf.save_to_disk(\n",
    "        dataset_path_hhrlhf\n",
    "    )\n",
    "    print(f\"Successfully uploaded dataset using alternative method to: {dataset_path_hhrlhf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203cb784-6fb6-4895-9e93-56cedd28581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_hhrlhf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f807dd-3e26-495e-b87d-c4aa930f6e08",
   "metadata": {},
   "source": [
    "## PPO training dataset\n",
    "\n",
    "Dataset used: Stanford Question&Answering Dataset (SQuAD) - https://rajpurkar.github.io/SQuAD-explorer/\n",
    "\n",
    "Target format: \n",
    "```json\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e1d57-2a86-4f0d-bacc-b37a506759e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SQuAD dataset\n",
    "!wget --no-check-certificate https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "!wget --no-check-certificate https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496165d3-809c-42b0-b288-63897c04730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "with open('./train-v2.0.json') as f:\n",
    "    d_train = json.load(f)\n",
    "with open('./dev-v2.0.json') as f:\n",
    "    d_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdac8b8-29ba-487e-9304-4233192782c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(dataset):\n",
    "    ret_questions = []\n",
    "    for topic in dataset:\n",
    "        paragraphs = topic['paragraphs']\n",
    "        for paragraph in paragraphs:\n",
    "            qas = paragraph['qas']\n",
    "            for qa in qas:\n",
    "                ret_questions.append([{\n",
    "            \"role\": \"system\", \"content\": f'Instruction: Please answer the user\\'s question to the best of your knowledge. If you don\\'t know the answer respond that you don\\'t know.',\n",
    "        }, {\n",
    "            \"role\": \"user\", \"content\": qa['question'],\n",
    "        }])\n",
    "    return ret_questions\n",
    "\n",
    "# Adjusting to llama prompt template format: https://github.com/meta-llama/llama-recipes\n",
    "def encode_dialogue_turn(message):\n",
    "    message = message\n",
    "    return f'<|start_header_id|>{message.get(\"role\")}<|end_header_id|>{message.get(\"content\")}<|eot_id|>'\n",
    "\n",
    "def encode_dialogue(dialogue):\n",
    "    return {'input': f'<|begin_of_text|>{functools.reduce(lambda a, b: a + encode_dialogue_turn(b), dialogue, \"\")}'}\n",
    "\n",
    "                                      \n",
    "def encode_dataset(dataset):\n",
    "    #print(dataset)\n",
    "    return list(map(encode_dialogue, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d113909-4d55-45c5-b9e1-ff5366c527d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = encode_dataset(extract_questions(d_train['data']))\n",
    "encoded_test = encode_dataset(extract_questions(d_test['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92ee57-c2d2-482d-bd18-306c3eafcd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add475d0-b7a8-4b52-a6af-bdf4fec2bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": Dataset.from_list(encoded_train),\n",
    "    \"test\": Dataset.from_list(encoded_test)\n",
    "})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf39dfc0-1806-47c8-959d-f83f06de7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict training context size (due to memory limitations, can be adjusted)\n",
    "input_min_text_length = 1\n",
    "input_max_text_length = 2048\n",
    "\n",
    "def create_and_prepare_dataset(tokenizer, dataset):\n",
    "    \n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(example):\n",
    "        text_size = input_size()\n",
    "        example[\"input_ids\"] = tokenizer.encode(example[\"input\"])[:text_size]\n",
    "        example[\"query\"] = tokenizer.decode(example[\"input_ids\"])\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "        \n",
    "    dataset.set_format(\"torch\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "tokenized_dataset_squad = create_and_prepare_dataset(tokenizer, dataset_dict).remove_columns([\"input\"])\n",
    "tokenized_dataset_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d897c65-7360-4ee7-b04c-028c30dbe943",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_squad['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78fdaec-4f1e-49c9-a305-42c52b3e2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to s3\n",
    "#s3_bucket = \"***S3_BUCKET_NAME***\"\n",
    "\n",
    "#dataset_path_squad = f's3://{s3_bucket}/experiments-squad/train-test-contextwindow-padding-2048'\n",
    "#tokenized_dataset_squad.save_to_disk(dataset_path_squad)\n",
    "\n",
    "#print(f\"Uploaded dataset to: {dataset_path_squad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2b009-9752-4ead-a5f7-7a8be9c1c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "\n",
    "# Define the S3 path\n",
    "s3_bucket = bucket_name\n",
    "dataset_path_squad = f's3://{s3_bucket}/experiments-squad/train-test-contextwindow-padding-2048'\n",
    "\n",
    "# Verify S3 bucket permissions first\n",
    "s3_client = boto3.client('s3')\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=s3_bucket)\n",
    "    print(f\"Successfully connected to bucket: {s3_bucket}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to bucket: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Save the dataset to S3 using the appropriate filesystem\n",
    "try:\n",
    "    # Make sure you have the s3fs package installed\n",
    "    tokenized_dataset_squad.save_to_disk(\n",
    "        dataset_path_squad, \n",
    "        fs=S3FileSystem()\n",
    "    )\n",
    "    print(f\"Successfully uploaded dataset to: {dataset_path_squad}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading to S3: {e}\")\n",
    "    \n",
    "    # Alternative approach if the above fails\n",
    "    print(\"Trying alternative approach...\")\n",
    "    tokenized_dataset_squad.save_to_disk(\n",
    "        dataset_path_squad\n",
    "    )\n",
    "    print(f\"Successfully uploaded dataset using alternative method to: {dataset_path_squad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b79b74-175d-42d4-84cf-93de75ece939",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb6653-b4ef-4498-9941-16d4742f0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set path to config file for remote decorator\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc05ff-1e36-4aa6-97f8-da7bf3de82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ad305-2bb0-4ac5-be70-6e61e56aa331",
   "metadata": {},
   "source": [
    "## Reward model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c62175-24e3-4e59-8ee0-1a0f3ca70418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "    \n",
    "def find_all_linear_names(hf_model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in hf_model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)   \n",
    "\n",
    "# Start training with remote decorator (https://docs.aws.amazon.com/sagemaker/latest/dg/train-remote-decorator.html). Additional job config is being pulled in from config.yaml. \n",
    "@remote(keep_alive_period_in_seconds=3600, volume_size=100, job_name_prefix=f\"train-{model_id.split('/')[-1].replace('.', '-')}-reward\", use_torchrun=True, nproc_per_node=1, include_local_workdir=True)\n",
    "def train_fn(\n",
    "        model_name,\n",
    "        train_ds,\n",
    "        lora_r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=1,\n",
    "        fsdp=\"\",\n",
    "        fsdp_config=None,\n",
    "        chunk_size=10000,\n",
    "        gradient_checkpointing=False,\n",
    "        merge_weights=False,\n",
    "        seed=42,\n",
    "        token=None,\n",
    "        model_hub_repo_id=None,\n",
    "        range_train=None,\n",
    "        range_eval=None\n",
    "):\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Initialize Accelerator object handling distributed training\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    # Login to HuggingFace\n",
    "    if token is not None:\n",
    "        login(token)\n",
    "\n",
    "    # Load tokenizer. Padding side is \"left\" because focus needs to be on completion\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = \"left\")\n",
    "\n",
    "    # Set tokenizer's pad Token\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "\n",
    "    # Load data from S3\n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    dataset = load_from_disk(train_ds)  \n",
    "    \n",
    "    \n",
    "    # Allow for partial dataset training\n",
    "    if range_train:\n",
    "        train_dataset = dataset[\"train\"].select(range(range_train))\n",
    "    else: \n",
    "        train_dataset = dataset[\"train\"]\n",
    "  \n",
    "    if range_eval:\n",
    "        eval_dataset = dataset[\"test\"].select(range(range_eval))\n",
    "    else:\n",
    "        eval_dataset = dataset[\"test\"]\n",
    "\n",
    "    # Specify quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        quant_storage_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load model with classification head for reward\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        #num_labels=1,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "        #attn_implementation=\"flash_attention_2\",\n",
    "        use_cache=False if gradient_checkpointing else True,\n",
    "        cache_dir=\"/tmp/.cache\"\n",
    "    )\n",
    "    \n",
    "    # Pre-LoRA trainable paremeters\n",
    "    print_trainable_parameters(model)     \n",
    "    \n",
    "    # Set model pad token id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    # Prepare model for quantized training\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=gradient_checkpointing)\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Get lora target modules\n",
    "    modules = find_all_linear_names(model)\n",
    "    print(f\"Found {len(modules)} modules to quantize: {modules}\")\n",
    "    \n",
    "    # Specify LoRA config\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    \n",
    "    # Make sure to not train for CLM\n",
    "    if config.task_type != \"SEQ_CLS\":\n",
    "        warnings.warn(\n",
    "            \"You are using a `task_type` that is different than `SEQ_CLS` for PEFT. This will lead to silent bugs\"\n",
    "            \" Make sure to pass --lora_task_type SEQ_CLS when using this script.\"\n",
    "        )\n",
    "    \n",
    "    # Create PeftModel\n",
    "    model = get_peft_model(model, config)\n",
    "    \n",
    "    # Post-LoRA trainable paremeters\n",
    "    print_trainable_parameters(model)     \n",
    "    \n",
    "    # Specify training config\n",
    "    reward_config = RewardConfig(\n",
    "                        per_device_train_batch_size=per_device_train_batch_size,\n",
    "                        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "                        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                        gradient_checkpointing=gradient_checkpointing,\n",
    "                        logging_strategy=\"steps\",\n",
    "                        logging_steps=100,\n",
    "                        log_on_each_node=False,\n",
    "                        num_train_epochs=num_train_epochs,\n",
    "                        learning_rate=learning_rate,\n",
    "                        bf16=True,\n",
    "                        ddp_find_unused_parameters=False,\n",
    "                        fsdp=fsdp,\n",
    "                        fsdp_config=fsdp_config,\n",
    "                        save_strategy=\"no\",\n",
    "                        output_dir=\"outputs\",\n",
    "                        max_length=512, \n",
    "                        remove_unused_columns=False,\n",
    "                        gradient_checkpointing_kwargs = {\"use_reentrant\": False}\n",
    "                        )\n",
    "    \n",
    "    # Initialize RewardTrainer object handling training\n",
    "    trainer = RewardTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=reward_config,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    \n",
    "    trainer.model.save_pretrained(\"/opt/ml/model\", safe_serialization=True)\n",
    "    \n",
    "    if model_hub_repo_id is not None:\n",
    "        trainer.model.push_to_hub(repo_id=model_hub_repo_id)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        tokenizer.save_pretrained(\"/opt/ml/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545db913-7b7c-4189-a860-0f68f6774d60",
   "metadata": {},
   "source": [
    "Define the Hugging Face repository ID for pushing the reward model adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a166e-9215-473d-b0c9-ce04ba8afc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_adapter_hub_repo_id = \"aristsakpinisaws/llama-31-hhrlhf-reward-adapter\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603eefb2-0060-4d19-8f3d-cd340335eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training job\n",
    "train_fn(\n",
    "    model_id,\n",
    "    train_ds=dataset_path_hhrlhf,  # Use S3 path instead of in-memory dataset\n",
    "    #train_ds=tokenized_dataset_hhrlhf,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=1,\n",
    "    token=hf_token,\n",
    "    model_hub_repo_id=rm_adapter_hub_repo_id,\n",
    "    range_train=100,\n",
    "    range_eval=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df334a-2e83-41d1-b75e-07d903b8c0a8",
   "metadata": {},
   "source": [
    "## Preference Alignment training with multi-adapter PPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33d9f0-b84d-44e9-9920-5f94f19e7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set path to config file for remote decorator\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e2e96-02cc-44bd-a389-f490eb5ae94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "# Start training with remote decorator (https://docs.aws.amazon.com/sagemaker/latest/dg/train-remote-decorator.html). Additional job config is being pulled in from config.yaml. \n",
    "@remote(keep_alive_period_in_seconds=3600, volume_size=100, job_name_prefix=f\"train-{model_id.split('/')[-1].replace('.', '-')}-multi-adapter-ppo\", use_torchrun=True, nproc_per_node=1)\n",
    "def train_fn(\n",
    "        model_name,\n",
    "        train_ds,\n",
    "        rm_adapter,\n",
    "        s3_output_path,\n",
    "        log_with=None,\n",
    "        use_safetensors=None,\n",
    "        use_score_scaling=False,\n",
    "        use_score_norm=False,\n",
    "        score_clip=None,\n",
    "        seed=42,\n",
    "        token=None,\n",
    "        model_hub_repo_id=None,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        gradient_checkpointing=True,\n",
    "        num_train_epochs=1,\n",
    "        merge_weights=True,\n",
    "        range_train=None,\n",
    "        ):\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Initialize Accelerator object handling distributed training\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Login to HuggingFace \n",
    "    if token is not None:\n",
    "        login(token)\n",
    "        \n",
    "    # Load tokenizer. Padding side is \"left\" because focus needs to be on completion\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "    # Set tokenizer's pad Token\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id  \n",
    "    \n",
    "    \n",
    "    # Load data from S3\n",
    "    dataset = load_from_disk(train_ds)\n",
    "    \n",
    "    \n",
    "    # Allow for partial dataset training\n",
    "    if range_train:\n",
    "        train_dataset = dataset[\"train\"].select(range(range_train))\n",
    "    else: \n",
    "        train_dataset = dataset[\"train\"]\n",
    "    \n",
    "    # Specify LoRA config\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    # Specify quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        model_name,\n",
    "        #device_map='auto',\n",
    "        peft_config=lora_config,\n",
    "        quantization_config=bnb_config,\n",
    "        reward_adapter=rm_adapter,\n",
    "        use_safetensors=use_safetensors,\n",
    "        #attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    \n",
    "    # Set model pad token id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "        \n",
    "    # Trainable paremeters\n",
    "    print_trainable_parameters(model)    \n",
    "\n",
    "    def collator(data):\n",
    "        return {key: [d[key] for d in data] for key in data[0]}\n",
    "\n",
    "    # Specify PPO training config\n",
    "    config = PPOConfig(\n",
    "        model_name,\n",
    "        log_with=None,\n",
    "        learning_rate=1e-5,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        mini_batch_size=1,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optimize_cuda_cache=True,\n",
    "        seed=42,\n",
    "        use_score_scaling=False,\n",
    "        use_score_norm=False,\n",
    "        score_clip=None,\n",
    "    )\n",
    "\n",
    "    # Initialize PPOTrainer object handling training\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config,\n",
    "        model,\n",
    "        ref_model=None,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=train_dataset,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    # Specifying inference params\n",
    "    generation_kwargs = {\n",
    "        #\"top_k\": 0.0,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"max_new_tokens\": 32,\n",
    "    }\n",
    "    \n",
    "    step = 0\n",
    "\n",
    "    for _epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "        \n",
    "        question_tensors = batch[\"input_ids\"]\n",
    "        \n",
    "        # Inference through model being fine-tuned\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            question_tensors,\n",
    "            return_prompt=False,\n",
    "            **generation_kwargs,\n",
    "        )\n",
    "        \n",
    "        # Decode response\n",
    "        batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "        \n",
    "        # Concat query and response\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        \n",
    "        # Tokenize query - response pair\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(ppo_trainer.accelerator.device)\n",
    "        \n",
    "        # Compute reward score\n",
    "        raw_rewards = ppo_trainer.accelerator.unwrap_model(ppo_trainer.model).compute_reward_score(**inputs)\n",
    "        rewards = [raw_rewards[i, -1, 1] for i in range(len(raw_rewards))]  # take last token\n",
    "\n",
    "        # Run PPO step\n",
    "        stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "        \n",
    "        step = step + 1      \n",
    "\n",
    "\n",
    "    if merge_weights:\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            \n",
    "            output_dir = \"/tmp/model\"\n",
    "    \n",
    "\n",
    "            ppo_trainer.save_pretrained(output_dir, safe_serialization=True)\n",
    "\n",
    "       \n",
    "            # clear memory\n",
    "            del model\n",
    "            del ppo_trainer\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # load PEFT model\n",
    "            model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "                output_dir,\n",
    "                torch_dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                use_cache=True,\n",
    "                cache_dir=\"/tmp/.cache\",\n",
    "            )\n",
    "\n",
    "            # Merge LoRA and base model and save\n",
    "            model = model.merge_and_unload()\n",
    "            model.save_pretrained(\n",
    "                os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\"),\n",
    "                safe_serialization=True,\n",
    "                max_shard_size=\"2GB\"\n",
    "            )\n",
    "            if model_hub_repo_id is not None:\n",
    "                model.push_to_hub(repo_id=model_hub_repo_id)\n",
    "\n",
    "            tokenizer.save_pretrained(os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\"))\n",
    "\n",
    "            if model_hub_repo_id is not None:\n",
    "                tokenizer.push_to_hub(repo_id=model_hub_repo_id)\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    else:\n",
    "        if accelerator.is_main_process:\n",
    "            \n",
    "            ppo_trainer.model.module.save_pretrained(\n",
    "                os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\"),\n",
    "                safe_serialization=True\n",
    "            )\n",
    "    \n",
    "            if model_hub_repo_id is not None:\n",
    "                ppo_trainer.push_to_hub(repo_id=model_hub_repo_id)\n",
    "    \n",
    "    \n",
    "            tokenizer.save_pretrained(os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\"))\n",
    "    \n",
    "            if model_hub_repo_id is not None:\n",
    "                tokenizer.push_to_hub(repo_id=model_hub_repo_id)\n",
    "\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        # Upload the model files to S3\n",
    "       \n",
    "        \n",
    "        # Get the S3 output path from the environment variables\n",
    "        # SageMaker automatically sets these environment variables\n",
    "        if os.environ.get(\"SM_MODEL_DIR\") and s3_output_path:\n",
    "            model_dir = os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\")\n",
    "            \n",
    "            print(f\"Uploading model from {model_dir} to {s3_output_path}\")\n",
    "            \n",
    "            # Initialize S3 client\n",
    "            s3_client = boto3.client('s3')\n",
    "            \n",
    "            # Extract bucket name and prefix from S3 URI\n",
    "            s3_uri_parts = s3_output_path.replace(\"s3://\", \"\").split(\"/\")\n",
    "            bucket_name = s3_uri_parts[0]\n",
    "            prefix = \"/\".join(s3_uri_parts[1:]) if len(s3_uri_parts) > 1 else \"\"\n",
    "            \n",
    "            # Walk through all files in the model directory and upload them\n",
    "            for root, dirs, files in os.walk(model_dir):\n",
    "                for file in files:\n",
    "                    local_path = os.path.join(root, file)\n",
    "                    # Create relative path to maintain directory structure\n",
    "                    relative_path = os.path.relpath(local_path, model_dir)\n",
    "                    s3_key = os.path.join(prefix, relative_path)\n",
    "                    \n",
    "                    print(f\"Uploading {local_path} to s3://{bucket_name}/{s3_key}\")\n",
    "                    try:\n",
    "                        s3_client.upload_file(local_path, bucket_name, s3_key)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to upload {local_path} to S3: {e}\")\n",
    "            \n",
    "            print(\"Model upload to S3 completed\")\n",
    "\n",
    "    # Wait for all processes to complete\n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "    \n",
    "    #if accelerator.is_main_process:\n",
    "\n",
    "    #    ppo_trainer.save_pretrained(\"/opt/ml/model\", safe_serialization=True)\n",
    "\n",
    "     #   if model_hub_repo_id is not None:\n",
    "      #      ppo_trainer.push_to_hub(repo_id=model_hub_repo_id)\n",
    "       #     tokenizer.push_to_hub(repo_id=model_hub_repo_id)\n",
    "\n",
    "    #with accelerator.main_process_first():\n",
    "     #   tokenizer.save_pretrained(\"/opt/ml/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a58cbd-ac72-4f13-b1c3-f1c208a15198",
   "metadata": {},
   "source": [
    "Let's point ot the adapter previously created and pushed in the Hugging Face Model Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802e652-ad6d-4421-908d-e6f5eae4d891",
   "metadata": {},
   "source": [
    "Define the Hugging Face repository ID for pushing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941a1b3-8bbe-44df-b18f-4c6fb5e82de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_hub_repo_id = \"aristsakpinisaws/llama-31-hhrlhf-squad-rlhf-adapter\"\n",
    "model_hub_repo_id = \"aristsakpinisaws/llama-31-hhrlhf-squad-rlhf-policy-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf52e40-0e75-404d-aa3e-c44810a09fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782dc1a-67cd-4b3a-9ae0-16dfa3e57775",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn(\n",
    "    model_id,\n",
    "    train_ds=dataset_path_squad,  # Use S3 path instead of in-memory dataset\n",
    "    #train_ds=tokenized_dataset_squad,\n",
    "    s3_output_path=s3_output_path,\n",
    "    rm_adapter=rm_adapter_hub_repo_id,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=1,\n",
    "    token=hf_token,\n",
    "    model_hub_repo_id=model_hub_repo_id,\n",
    "    range_train=50,\n",
    "    merge_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41292d-e8f3-4c74-98c0-4c55af7a3441",
   "metadata": {},
   "source": [
    "# Import fine-tuned model to Amazon Bedrock through Custom Model Import (CMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad3ffbf-451c-43bd-a39a-1bbaa666fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize IAM client\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "# Role name - consider making this unique if you create multiple roles\n",
    "role_name = \"BedrockCustomModelImportRole\"\n",
    "\n",
    "# Define trust policy to allow Bedrock to assume this role\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define permissions policy for Bedrock model import\n",
    "bedrock_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::*/*\",\n",
    "                \"arn:aws:s3:::*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateModelCustomizationJob\",\n",
    "                \"bedrock:CreateModelImportJob\",\n",
    "                \"bedrock:GetModelCustomizationJob\",\n",
    "                \"bedrock:GetModelImportJob\",\n",
    "                \"bedrock:StopModelCustomizationJob\",\n",
    "                \"bedrock:StopModelImportJob\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"kms:Decrypt\",\n",
    "                \"kms:GenerateDataKey\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"cloudwatch:PutMetricData\"\n",
    "            ],\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"cloudwatch:namespace\": \"AWS/Bedrock\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/bedrock/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create or update the role\n",
    "try:\n",
    "    # Check if role exists\n",
    "    try:\n",
    "        iam.get_role(RoleName=role_name)\n",
    "        print(f\"Role {role_name} already exists. Updating policies...\")\n",
    "        # Delete any existing policies to ensure clean slate\n",
    "        for policy in iam.list_attached_role_policies(RoleName=role_name)['AttachedPolicies']:\n",
    "            iam.detach_role_policy(RoleName=role_name, PolicyArn=policy['PolicyArn'])\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'NoSuchEntity':\n",
    "            # Create role with trust policy if it doesn't exist\n",
    "            print(f\"Creating new role: {role_name}\")\n",
    "            iam.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "                Description=\"Role for Amazon Bedrock Model Import operations\"\n",
    "            )\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Create policy for Bedrock operations\n",
    "    policy_name = f\"{role_name}Policy\"\n",
    "    \n",
    "    # Check if policy exists and delete if it does\n",
    "    try:\n",
    "        existing_policy = iam.get_policy(PolicyArn=f\"arn:aws:iam::{boto3.client('sts').get_caller_identity()['Account']}:policy/{policy_name}\")\n",
    "        \n",
    "        # Detach policy if attached to our role\n",
    "        try:\n",
    "            iam.detach_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyArn=existing_policy['Policy']['Arn']\n",
    "            )\n",
    "        except ClientError:\n",
    "            pass  # Policy may not be attached\n",
    "        \n",
    "        # Delete existing versions (except default)\n",
    "        policy_versions = iam.list_policy_versions(PolicyArn=existing_policy['Policy']['Arn'])['Versions']\n",
    "        for version in policy_versions:\n",
    "            if not version['IsDefaultVersion']:\n",
    "                iam.delete_policy_version(\n",
    "                    PolicyArn=existing_policy['Policy']['Arn'],\n",
    "                    VersionId=version['VersionId']\n",
    "                )\n",
    "        \n",
    "        # Create new version and set as default\n",
    "        iam.create_policy_version(\n",
    "            PolicyArn=existing_policy['Policy']['Arn'],\n",
    "            PolicyDocument=json.dumps(bedrock_policy_document),\n",
    "            SetAsDefault=True\n",
    "        )\n",
    "        policy_arn = existing_policy['Policy']['Arn']\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'NoSuchEntity':\n",
    "            # Create new policy\n",
    "            response = iam.create_policy(\n",
    "                PolicyName=policy_name,\n",
    "                PolicyDocument=json.dumps(bedrock_policy_document),\n",
    "                Description=\"Permissions for Amazon Bedrock Model Import operations\"\n",
    "            )\n",
    "            policy_arn = response['Policy']['Arn']\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Attach policy to role\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=policy_arn\n",
    "    )\n",
    "    \n",
    "    # Wait for policy to propagate\n",
    "    print(f\"Waiting for IAM role and policies to propagate...\")\n",
    "    time.sleep(10)  # IAM changes can take a few seconds to propagate\n",
    "    \n",
    "    # Get the role ARN\n",
    "    role = iam.get_role(RoleName=role_name)\n",
    "    role_arn = role['Role']['Arn']\n",
    "    \n",
    "    print(f\"Successfully configured role: {role_arn}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up IAM role: {str(e)}\")\n",
    "    role_arn = None  # Set to None if there was an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f88673-d1ce-4b53-a93e-da620e68c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client('bedrock', region_name=Session().boto_region_name)\n",
    "role_arn = \"arn:aws:iam::308819823671:role/service-role/executionRoleName-20250213T234639\"\n",
    "\n",
    "# Define name for imported model\n",
    "imported_model_name = f'llama-31-hhrlhf-squad-rlhf-policy-model-{int(time.time())}'\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=imported_model_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=role_arn,\n",
    "    modelDataSource={\n",
    "        's3DataSource': {\n",
    "            's3Uri': s3_output_path\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "job_Arn = response['jobArn']\n",
    "\n",
    "# Output the job ARN\n",
    "print(f\"Model import job created with ARN: {response['jobArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bb7e6-d979-45a0-af06-05fbee5c22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CMI job status\n",
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_Arn)\n",
    "    status = response['status'].upper()\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)  # Check every 60 seconds\n",
    "\n",
    "# Get the model ID\n",
    "model_arn = response['importedModelArn']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7faf1-795f-4a62-8780-b03013c54849",
   "metadata": {},
   "source": [
    "# Serverless model inference with imported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e945ed2-fbce-4658-903b-d7c35b6a5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2',\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,     # 5 minutes\n",
    "        retries={'max_attempts': 3}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386e6b2-c842-4f8f-bb1f-beeee1bb1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(messages, temperature=0.3, max_tokens=4096, top_p=0.9, continuation=False, max_retries=10):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "    \n",
    "    Parameters:\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        continuation (bool): Whether this is a continuation of previous generation\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, \n",
    "                                         add_generation_prompt=not continuation)\n",
    "    \n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            response = client.invoke_model(\n",
    "                modelId=model_arn,\n",
    "                body=json.dumps({\n",
    "                    'prompt': prompt,\n",
    "                    'temperature': temperature,\n",
    "                    'max_gen_len': max_tokens,\n",
    "                    'top_p': top_p\n",
    "                }),\n",
    "                accept='application/json',\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response['body'].read().decode('utf-8'))\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            attempt += 1\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(30)\n",
    "    \n",
    "    raise Exception(\"Failed to get response after maximum retries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8889a-f065-42e3-a2be-eb459f30596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"Hi, how are you?\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "response = generate(messages)\n",
    "print(\"Model Response:\")\n",
    "print(response[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97778b-4152-48b2-8427-e0eba0dcef16",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0f2cd-6ad2-4813-8d5c-0b053486bdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
