{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e497d314-b456-4261-a7ad-3e9040e010af",
   "metadata": {},
   "source": [
    "# Frugal Reinforcement Learning from Human Feedback (RLHF) with Meta Llama models: Preference aligning LLMs with Multi-Adapter PPO\n",
    "\n",
    "<img src=\"./images/Llama.png\" width=\"30%\" alt='Llama.png'/> \n",
    "\n",
    "This workshop will guide you through the process of fine-tuning Large Language Models (LLMs) using Reinforcement Learning from Human Feedback (RLHF) with a multi-adapter Proximal Policy Optimization (PPO) approach.\n",
    "\n",
    "## Workshop Overview\n",
    "\n",
    "In this tutorial, you will:\n",
    "1. Set up the necessary environment and dependencies\n",
    "2. Prepare datasets for both reward model training and PPO fine-tuning\n",
    "3. Train a reward model using human preference data\n",
    "4. Perform PPO-based RLHF training using your reward model\n",
    "5. Deploy the fine-tuned model to Amazon Bedrock for inference\n",
    "6. Clean up resources\n",
    "\n",
    "More information on LLM fine-tuning, benefits of reinforcement learning approaches like RLHF and the benefits or multi-adapter PPO can be found in [this blogpost](https://medium.com/data-science/preference-alignment-for-everyone-2563cec4d10e). \n",
    "\n",
    "## Scenario\n",
    "\n",
    "While most of the models published these days have already gone through multiple fine-tuning steps like SFT or even PA, since these models are general purpose ones they where certainly not performed tailored to your target users or target domain. This means that even though we are using a pre-aligned model (e.g. an instruction fine-tuned model), for optimising model performance in your domain further alignment steps are required.\n",
    "\n",
    "For this blog we will assume the model should be optimised towards maximising the helpfulness while carrying out user-facing single- and multi-turn conversations in a Q&A style in the scientific domain. Thus, we will start from a general-purpose instruct / Q&A pre-trained FM.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, you need to:\n",
    "\n",
    "1. Accept the license terms for [Meta Llama 3.1 8B](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) and [Meta Llama 3.2 1B](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) Instruct models on HuggingFace. Therefor navigate to the respective model pages through the links.\n",
    "\n",
    "<img src=\"./images/LlamaLicenseAgreement.png\" width=\"30%\" alt='LlamaLicenseAgreement.png'/> \n",
    "\n",
    "2. Create an access token for HuggingFace authentication. Therefor please follow the instructions from the [HuggingFace documentation](https://huggingface.co/docs/hub/en/security-tokens).\n",
    "\n",
    "<img src=\"./images/CreateHFToken.png\" width=\"30%\" alt='CreateHFToken.png'/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9067e",
   "metadata": {},
   "source": [
    "## Step 1: Model Selection\n",
    "\n",
    "In this workshop, we'll be using a Meta Llama model for our fine-tuning process. You can choose between:\n",
    "- Meta Llama-3.1-8B-Instruct: More powerful but requires more compute resources (training with ml.g5.12xlarge, configure in config.json)\n",
    "- Meta Llama-3.2-1B-Instruct: Smaller model, faster training, lower resource requirements (training with ml.g5.2xlarge or ml.p3.2xlarge, configure in config.json)\n",
    "\n",
    "Choose the model that best fits your available compute resources. For workshop settings with limited time or resources, the 1B model is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260e672-e033-4470-8683-cedeca74ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model to be fine-tuned\n",
    "#model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca8a06",
   "metadata": {},
   "source": [
    "## Step 2: Environment Setup\n",
    "\n",
    "Before we begin our RLHF training process, we need to set up our environment with all necessary packages and dependencies. We'll be using PyTorch with CUDA support for GPU acceleration and several specialized libraries for training and optimizing our language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e592547-c863-45e8-afce-d0a613e6f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "%pip install -U torch==2.2.0+cu118 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176f2bf-40e4-4b69-b42b-0be989ed8caf",
   "metadata": {},
   "source": [
    "The remaining dependencies are managed in a `requirements.txt` which will be shared by the notebook environment and the remote training jobs to ensure cross-compatability of dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1114a-caf0-4625-a90a-9b120b4a393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies from requirements.txt file\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179940a8",
   "metadata": {},
   "source": [
    "## Step 3: Import Libraries\n",
    "\n",
    "Now we'll import all the libraries we need for the RLHF workflow. This includes:\n",
    "- AWS SDK (boto3) for storage and compute management\n",
    "- PyTorch for deep learning operations\n",
    "- Transformers from Hugging Face for model loading and tokenization\n",
    "- datasets for working with Hugging Face datasets\n",
    "- TRL (Transformer Reinforcement Learning) for RLHF implementation\n",
    "- PEFT (Parameter-Efficient Fine-Tuning) for adapter-based training\n",
    "- accelerate for distributed training\n",
    "- Other utilities for data handling and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4605a5-553e-409b-b898-9d10b08a7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import botocore\n",
    "import bitsandbytes as bnb\n",
    "import multiprocessing\n",
    "import sys\n",
    "import functools\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
    "from trl import ModelConfig, RewardConfig, PPOConfig, PPOTrainer, RewardTrainer, AutoModelForCausalLMWithValueHead, get_kbit_device_map, get_peft_config, get_quantization_config\n",
    "from trl.core import LengthSampler\n",
    "from accelerate import Accelerator\n",
    "from peft import AutoPeftModelForCausalLM, AutoPeftModelForSequenceClassification, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from sagemaker.remote_function import remote\n",
    "from tqdm import tqdm\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a544fb-a90f-48f7-a49e-5f41496a1d65",
   "metadata": {},
   "source": [
    "## Step 4: Setting Up Storage Infrastructure\n",
    "\n",
    "For our training workflow, we need a place to store our datasets and model artifacts. We'll create an Amazon S3 bucket that will serve as our central storage repository throughout the workshop. \n",
    "\n",
    "The function below will:\n",
    "1. Create a uniquely named S3 bucket \n",
    "2. Set up directories for model artifacts and datasets\n",
    "3. Configure appropriate permissions for access\n",
    "\n",
    "This setup ensures we have a persistent storage location for our data and models, which is especially important for distributed training workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1adfa-58ad-4c14-80e9-95522c313cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "def create_s3_bucket_for_models(bucket_name=None, region=None):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket for storing SageMaker models.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Optional name for the S3 bucket. If not provided, a name will be generated.\n",
    "        region: AWS region to create the bucket in. If not provided, uses the SageMaker session's region.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (bucket_name, s3_output_path) - The name of the created bucket and the S3 path for model output\n",
    "    \"\"\"\n",
    "    # Initialize boto3 clients\n",
    "    s3_client = boto3.client('s3')\n",
    "    session = Session()\n",
    "    \n",
    "    # Get the AWS region if not provided\n",
    "    if not region:\n",
    "        region = session.boto_region_name\n",
    "    \n",
    "    # Generate a unique bucket name if not provided\n",
    "    if not bucket_name:\n",
    "        timestamp = int(time.time())\n",
    "        random_suffix = str(uuid.uuid4())[:8]\n",
    "        bucket_name = f\"sagemaker-model-artifacts-{timestamp}-{random_suffix}\"\n",
    "    \n",
    "    # Create the S3 bucket\n",
    "    try:\n",
    "        if region == 'us-east-1':\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration=location\n",
    "            )\n",
    "        print(f\"Created S3 bucket: {bucket_name}\")\n",
    "        \n",
    "        # Create a folder for model output\n",
    "        s3_output_path = f\"s3://{bucket_name}/models\"\n",
    "        s3_data_path = f\"s3://{bucket_name}/data\"\n",
    "        \n",
    "        return bucket_name, s3_output_path, s3_data_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating S3 bucket: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Create a bucket for your data and models\n",
    "bucket_name, s3_output_path, s3_data_path = create_s3_bucket_for_models()\n",
    "print(f\"Model output path: {s3_output_path}, Data output path: {s3_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f95925-216b-4cc5-bdfa-7f28c50e56ea",
   "metadata": {},
   "source": [
    "## Step 5: Data Preparation for Reward Model Training\n",
    "\n",
    "### Understanding Reward Model Data Format\n",
    "\n",
    "A key component of RLHF is the reward model, which learns from human preference data to score model outputs. For this workshop, we'll use the open-source Anthropic [HH-RLHF dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf), specifically focusing on the helpful subset.\n",
    "\n",
    "The reward model dataset needs to contain pairs of responses where one response is preferred over the other (chosen vs rejected). The model will learn to assign higher scores to the preferred responses.\n",
    "\n",
    "#### Target Format:\n",
    "The dataset should have the following structure for reward model training:\n",
    "\n",
    "\n",
    "```json\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
    "        num_rows: _\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
    "        num_rows: _\n",
    "    })\n",
    "})\n",
    "```\n",
    "\n",
    "Let's start by downloading and preprocessing this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83892c",
   "metadata": {},
   "source": [
    "### Step 5.1: HuggingFace Authentication\n",
    "\n",
    "First, we need to authenticate with HuggingFace Hub to access their datasets and models. Replace the placeholder with your own HuggingFace token.\n",
    "\n",
    "> **Important**: Your HuggingFace token should have write access if you intend to push models to the Hub later in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0b54c5-6e15-40d1-9dbc-8bcf75f2d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to huggingface\n",
    "hf_token = \"***HF_TOKEN***\"\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77f9a6-6a3d-4693-97dd-95864c1de3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ds = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"helpful-base\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175b469-eb3d-47f6-b2cc-f7ecf2eb8909",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][67]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70952262",
   "metadata": {},
   "source": [
    "### Step 5.2: Exploring the HH-RLHF Dataset\n",
    "\n",
    "Let's examine a sample from the dataset to understand its structure. The Anthropic HH-RLHF dataset contains pairs of AI assistant responses to user queries, with one response (chosen) being preferred by human evaluators over the other (rejected).\n",
    "\n",
    "Each example contains:\n",
    "- A human query or instruction\n",
    "- A chosen response that was preferred by human evaluators\n",
    "- A rejected response that was less preferred\n",
    "\n",
    "Understanding this structure is crucial as we'll need to transform it into our target format for the reward model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9920f02",
   "metadata": {},
   "source": [
    "### Step 5.3: Preprocessing the Dataset\n",
    "\n",
    "Now we need to process the dataset into a structured format. The `extract_dialogue` function will parse the text into a list of message dictionaries with \"role\" and \"content\" fields, following a chat template format.\n",
    "\n",
    "This preprocessing function:\n",
    "1. Parses each dialogue into user/assistant message pairs\n",
    "2. Extracts the initial prompt from the conversation\n",
    "3. Structures both chosen and rejected conversations in a consistent format\n",
    "\n",
    "This step is crucial because it transforms the raw text into a structured format that our models can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b3887-4558-4f52-9031-a0438f824077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dialogue(input_text):\n",
    "    # Split the input by lines and initialize variables\n",
    "    lines = input_text.strip().split(\"\\n\\n\")\n",
    "    dialogue_list = []\n",
    "\n",
    "    # Iterate through each line and extract the dialogue\n",
    "    for line in lines:\n",
    "        # Check if the line starts with \"Human\" or \"Assistant\" and split accordingly\n",
    "        if line.startswith(\"Human:\"):\n",
    "            role = \"user\"\n",
    "            content = line.replace(\"Human: \", \"\").strip()\n",
    "        elif line.startswith(\"Assistant:\"):\n",
    "            role = \"assistant\"\n",
    "            content = line.replace(\"Assistant: \", \"\").strip()\n",
    "        else:\n",
    "            # If the line doesn't start with \"Human\" or \"Assistant\", it's part of the previous message's content\n",
    "            # Append it to the last message's content\n",
    "            dialogue_list[-1][\"content\"] += \"\\n\\n\" + line.strip()\n",
    "            continue\n",
    "\n",
    "        # Append the extracted dialogue piece to the list\n",
    "        dialogue_list.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    return dialogue_list\n",
    "\n",
    "def process(row):\n",
    "        row[\"chosen\"] = extract_dialogue(row[\"chosen\"])\n",
    "        row[\"rejected\"] = extract_dialogue(row[\"rejected\"])\n",
    "        row[\"prompt\"] = row[\"chosen\"][0][\"content\"]\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec141d63-be1f-455f-b0ad-8bdd0f4d2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_processed = ds.map(\n",
    "        process,\n",
    "        load_from_cache_file=False,\n",
    "    )\n",
    "ds_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a51ee4-c025-4e3c-a8e4-c35b261d4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_processed['train'][67]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46c391",
   "metadata": {},
   "source": [
    "### Step 5.4: Applying Llama Chat Template\n",
    "\n",
    "Different LLM families use different formatting for chat interactions. Since we're working with Llama models, we need to convert our structured data into Llama's specific chat template format.\n",
    "\n",
    "The functions below:\n",
    "1. Define the system prompt that will guide the model's behavior\n",
    "2. Create helper functions to properly encode dialogue turns following Llama's format\n",
    "3. Apply the chat template to both chosen and rejected responses\n",
    "\n",
    "The formatted dialogues will include special tokens like `<|start_header_id|>`, `<|end_header_id|>`, and `<|eot_id|>` that Llama models recognize for chat interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9bb8e-5dc3-4089-b1eb-4a100629228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting to llama prompt template format: https://github.com/meta-llama/llama-recipes\n",
    "system_prompt = \"Please answer the user's question to the best of your knowledge. If you don't know the answer respond that you don't know.\"\n",
    "\n",
    "def encode_dialogue_turn(message):\n",
    "    return f'<|start_header_id|>{message.get(\"role\")}<|end_header_id|>{message.get(\"content\")}<|eot_id|>'\n",
    "\n",
    "def encode_dialogue(dialogue):\n",
    "    if system_prompt:\n",
    "        return f'<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|>{functools.reduce(lambda a, b: a + encode_dialogue_turn(b), dialogue, \"\")}'\n",
    "    else:\n",
    "        return f'<|begin_of_text|>{functools.reduce(lambda a, b: a + encode_dialogue_turn(b), dialogue, \"\")}'\n",
    "\n",
    "\n",
    "def encode_row(item):\n",
    "    return {\"chosen\": encode_dialogue(item[\"chosen\"]), \"rejected\": encode_dialogue(item[\"rejected\"]), \"prompt\": item[\"prompt\"]}\n",
    "                                      \n",
    "def encode_dataset(dataset):\n",
    "    return list(map(encode_row, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9822d-5fea-4a70-b04e-e532d8601128",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = ds_processed.map(encode_row)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b6a823-e0cd-480b-b2c1-2175af34ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset['train'][67]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed2e30",
   "metadata": {},
   "source": [
    "### Step 5.5: Loading the Tokenizer\n",
    "\n",
    "Now we need to load the tokenizer for our chosen model. The tokenizer converts text into token IDs that the model can process. It's important to use the tokenizer corresponding to the exact model we're fine-tuning to ensure compatible vocabulary and special tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cbd90f",
   "metadata": {},
   "source": [
    "### Step 5.6: Tokenizing the Dataset\n",
    "\n",
    "With our tokenizer loaded, we can now convert the text data into token IDs and attention masks that our model can process. For reward model training, we need to tokenize both the chosen and rejected responses.\n",
    "\n",
    "The `preprocess_function` will:\n",
    "1. Convert the chosen and rejected text into token IDs\n",
    "2. Generate attention masks for both sequences\n",
    "3. Return a dictionary with all four components required for our reward model\n",
    "\n",
    "This step transforms human-readable text into the numerical representation that neural networks process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40caca75-7b73-40db-a428-9b4a1d539981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff8d6eb-20b9-428a-9826-9cb57837bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and stack into target format\n",
    "def preprocess_function(examples):\n",
    "    new_examples = {\n",
    "        \"input_ids_chosen\": [],\n",
    "        \"attention_mask_chosen\": [],\n",
    "        \"input_ids_rejected\": [],\n",
    "        \"attention_mask_rejected\": [],\n",
    "    }\n",
    "    for chosen, rejected in zip(examples[\"chosen\"], examples[\"rejected\"]):\n",
    "        tokenized_chosen = tokenizer(chosen)\n",
    "        tokenized_rejected = tokenizer(rejected)\n",
    "\n",
    "        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
    "        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
    "\n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4661c7-d0bf-4763-afda-5554c44ee250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_dataset_hhrlhf = encoded_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "    ).remove_columns([\"chosen\", \"rejected\", \"prompt\"])\n",
    "tokenized_dataset_hhrlhf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c9aa4",
   "metadata": {},
   "source": [
    "### Step 5.7: Saving the Processed Dataset\n",
    "\n",
    "Now that we've prepared our dataset for reward model training, we need to save it to Amazon S3 for later use. This step ensures our processed data is:\n",
    "\n",
    "1. Persistently stored for future training runs\n",
    "2. Accessible to distributed training jobs\n",
    "3. Organized in a structured way for the entire RLHF workflow\n",
    "\n",
    "We'll use the S3FileSystem utility to efficiently upload the dataset to our S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6061c2-a93f-4a67-aa0b-e6ba0ccfb8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "#from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# Define the S3 path\n",
    "s3_bucket = bucket_name\n",
    "dataset_path_hhrlhf = f's3://{s3_bucket}/experiments-hhrlhf/helpful-base-train-test-tokenized-llama318binstruct'\n",
    "\n",
    "# Verify S3 bucket permissions first\n",
    "s3_client = boto3.client('s3')\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=s3_bucket)\n",
    "    print(f\"Successfully connected to bucket: {s3_bucket}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to bucket: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Save the dataset to S3 using the appropriate filesystem\n",
    "try:\n",
    "    # Make sure you have the s3fs package installed\n",
    "    tokenized_dataset_hhrlhf.save_to_disk(\n",
    "        dataset_path_hhrlhf, \n",
    "        fs=S3FileSystem()\n",
    "    )\n",
    "    print(f\"Successfully uploaded dataset to: {dataset_path_hhrlhf}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading to S3: {e}\")\n",
    "    \n",
    "    # Alternative approach if the above fails\n",
    "    print(\"Trying alternative approach...\")\n",
    "    #fs = s3fs.S3FileSystem()\n",
    "    tokenized_dataset_hhrlhf.save_to_disk(\n",
    "        dataset_path_hhrlhf\n",
    "    )\n",
    "    print(f\"Successfully uploaded dataset using alternative method to: {dataset_path_hhrlhf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203cb784-6fb6-4895-9e93-56cedd28581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_hhrlhf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f807dd-3e26-495e-b87d-c4aa930f6e08",
   "metadata": {},
   "source": [
    "## Step 6: Data Preparation for PPO Training\n",
    "\n",
    "### Understanding PPO Training Data Requirements\n",
    "\n",
    "For the Proximal Policy Optimization (PPO) phase of RLHF, we need a dataset of prompts that will be used to generate responses which will then be scored by our reward model. For this workshop, we'll use the [Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/) to provide diverse and engaging prompts for our model.\n",
    "\n",
    "Unlike reward model training, PPO training doesn't need paired responses. Instead, it needs:\n",
    "1. High-quality prompts for the model to respond to\n",
    "2. A reward model to score the generated responses\n",
    "3. A reference model to prevent policy drift\n",
    "\n",
    "Let's prepare the SQuAD dataset for our PPO training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855375e",
   "metadata": {},
   "source": [
    "### Step 6.1: Downloading the SQuAD Dataset\n",
    "\n",
    "First, we need to download the SQuAD dataset, which contains diverse question-answer pairs across a range of topics. This dataset is perfect for PPO training as it provides natural, well-formed questions that users might ask an AI assistant.\n",
    "\n",
    "The dataset should have the following structure for our PPO model training:\n",
    "\n",
    "```json\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['input_ids', 'query'],\n",
    "        num_rows: ...\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['input_ids', 'query'],\n",
    "        num_rows: ...\n",
    "    })\n",
    "})\n",
    "```\n",
    "\n",
    "We'll download both the training and development sets to create our train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e1d57-2a86-4f0d-bacc-b37a506759e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SQuAD dataset\n",
    "!wget --no-check-certificate https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "!wget --no-check-certificate https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3735d",
   "metadata": {},
   "source": [
    "### Step 6.2: Loading the SQuAD Data Files\n",
    "\n",
    "Now we'll load the downloaded JSON files into Python objects. SQuAD has a specific structure with topics, paragraphs, questions, and answers. We'll need to extract just the questions for our PPO prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496165d3-809c-42b0-b288-63897c04730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "with open('./train-v2.0.json') as f:\n",
    "    d_train = json.load(f)\n",
    "with open('./dev-v2.0.json') as f:\n",
    "    d_test = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1172800e",
   "metadata": {},
   "source": [
    "### Step 6.3: Processing the SQuAD Dataset\n",
    "\n",
    "We need to extract questions from the SQuAD dataset and format them for our PPO training. The functions below will:\n",
    "\n",
    "1. Extract questions from the nested SQuAD structure\n",
    "2. Format each question with a system instruction and user message\n",
    "3. Convert the structured messages into Llama's chat format\n",
    "4. Package everything in a format suitable for the PPO training phase\n",
    "\n",
    "This preprocessing transforms a QA dataset into prompts for generative responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdac8b8-29ba-487e-9304-4233192782c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(dataset):\n",
    "    ret_questions = []\n",
    "    for topic in dataset:\n",
    "        paragraphs = topic['paragraphs']\n",
    "        for paragraph in paragraphs:\n",
    "            qas = paragraph['qas']\n",
    "            for qa in qas:\n",
    "                ret_questions.append([{\n",
    "            \"role\": \"system\", \"content\": f'Instruction: Please answer the user\\'s question to the best of your knowledge. If you don\\'t know the answer respond that you don\\'t know.',\n",
    "        }, {\n",
    "            \"role\": \"user\", \"content\": qa['question'],\n",
    "        }])\n",
    "    return ret_questions\n",
    "\n",
    "# Adjusting to llama prompt template format: https://github.com/meta-llama/llama-recipes\n",
    "def encode_dialogue_turn(message):\n",
    "    message = message\n",
    "    return f'<|start_header_id|>{message.get(\"role\")}<|end_header_id|>{message.get(\"content\")}<|eot_id|>'\n",
    "\n",
    "def encode_dialogue(dialogue):\n",
    "    return {'input': f'<|begin_of_text|>{functools.reduce(lambda a, b: a + encode_dialogue_turn(b), dialogue, \"\")}'}\n",
    "\n",
    "                                      \n",
    "def encode_dataset(dataset):\n",
    "    #print(dataset)\n",
    "    return list(map(encode_dialogue, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d113909-4d55-45c5-b9e1-ff5366c527d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = encode_dataset(extract_questions(d_train['data']))\n",
    "encoded_test = encode_dataset(extract_questions(d_test['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c13ec",
   "metadata": {},
   "source": [
    "### Step 6.4: Examining the Processed Data\n",
    "\n",
    "Let's look at the first example in our processed dataset to verify it has the correct format. Each example should contain an `input` field with the formatted prompt in Llama's chat template format. This is what the model will use to generate responses during PPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92ee57-c2d2-482d-bd18-306c3eafcd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8e023",
   "metadata": {},
   "source": [
    "### Step 6.5: Creating the PPO Dataset Structure\n",
    "\n",
    "To make our dataset compatible with the PPO training workflow, we'll organize it into a HuggingFace `DatasetDict` with train and test splits. This structure allows for easy batch processing and evaluation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add475d0-b7a8-4b52-a6af-bdf4fec2bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": Dataset.from_list(encoded_train),\n",
    "    \"test\": Dataset.from_list(encoded_test)\n",
    "})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d102c5e",
   "metadata": {},
   "source": [
    "### Step 6.6: Tokenizing the PPO Training Dataset\n",
    "\n",
    "Now we need to tokenize our prompts for the PPO training process. For this training phase, we'll:\n",
    "\n",
    "1. Limit the context window to manage memory usage (1-2048 tokens)\n",
    "2. Encode each example as token IDs\n",
    "3. Create a query field that contains the decoded input for debugging\n",
    "4. Format everything as PyTorch tensors for efficient GPU training\n",
    "\n",
    "This step ensures our data is ready for efficient processing during the PPO training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf39dfc0-1806-47c8-959d-f83f06de7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict training context size (due to memory limitations, can be adjusted)\n",
    "input_min_text_length = 1\n",
    "input_max_text_length = 2048\n",
    "\n",
    "def create_and_prepare_dataset(tokenizer, dataset):\n",
    "    \n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(example):\n",
    "        text_size = input_size()\n",
    "        example[\"input_ids\"] = tokenizer.encode(example[\"input\"])[:text_size]\n",
    "        example[\"query\"] = tokenizer.decode(example[\"input_ids\"])\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "        \n",
    "    dataset.set_format(\"torch\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "tokenized_dataset_squad = create_and_prepare_dataset(tokenizer, dataset_dict).remove_columns([\"input\"])\n",
    "tokenized_dataset_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d897c65-7360-4ee7-b04c-028c30dbe943",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_squad['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f92dda",
   "metadata": {},
   "source": [
    "### Step 6.7: Saving the PPO Dataset\n",
    "\n",
    "Finally, we'll save our processed PPO dataset to Amazon S3 for use in the training phase. This makes the dataset:\n",
    "\n",
    "1. Persistently available for the PPO training job\n",
    "2. Accessible for distributed training\n",
    "3. Properly organized in our project structure\n",
    "\n",
    "Note that the commented-out code shows an alternative approach for saving to S3. We'll use a more robust method in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2b009-9752-4ead-a5f7-7a8be9c1c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "\n",
    "# Define the S3 path\n",
    "s3_bucket = bucket_name\n",
    "dataset_path_squad = f's3://{s3_bucket}/experiments-squad/train-test-contextwindow-padding-2048'\n",
    "\n",
    "# Verify S3 bucket permissions first\n",
    "s3_client = boto3.client('s3')\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=s3_bucket)\n",
    "    print(f\"Successfully connected to bucket: {s3_bucket}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to bucket: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Save the dataset to S3 using the appropriate filesystem\n",
    "try:\n",
    "    # Make sure you have the s3fs package installed\n",
    "    tokenized_dataset_squad.save_to_disk(\n",
    "        dataset_path_squad, \n",
    "        fs=S3FileSystem()\n",
    "    )\n",
    "    print(f\"Successfully uploaded dataset to: {dataset_path_squad}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading to S3: {e}\")\n",
    "    \n",
    "    # Alternative approach if the above fails\n",
    "    print(\"Trying alternative approach...\")\n",
    "    tokenized_dataset_squad.save_to_disk(\n",
    "        dataset_path_squad\n",
    "    )\n",
    "    print(f\"Successfully uploaded dataset using alternative method to: {dataset_path_squad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d192fa6",
   "metadata": {},
   "source": [
    "## Step 7: RLHF Training\n",
    "\n",
    "Now that we have prepared our datasets, we'll implement the two-phase RLHF training process (steps 2 and 4 in below illustration):\n",
    "\n",
    "<img src=\"./images/MultiAdapterPPOprocess.png\" width=\"50%\" alt='MultiAdapterPPOprocess.png'/> \n",
    "\n",
    "1. **Step 2: Reward Model Training**: Train a model to learn from human preferences\n",
    "2. **Step 4: PPO Training**: Use the reward model to guide policy optimization of our LLM\n",
    "\n",
    "Each phase requires careful configuration of hyperparameters and training infrastructure. We'll use parameter-efficient fine-tuning (PEFT) with LoRA adapters to make the process computationally efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb6653-b4ef-4498-9941-16d4742f0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set path to config file for remote decorator\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc05ff-1e36-4aa6-97f8-da7bf3de82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d8a18",
   "metadata": {},
   "source": [
    "## Step 7.1: Understanding the Reward Model Training Function\n",
    "\n",
    "Now we'll implement the reward model training process using parameter-efficient fine-tuning (PEFT) with LoRA adapters. Let's break down the `train_fn` function:\n",
    "\n",
    "### Key Components:\n",
    "1. **Model Quantization**: We use 4-bit quantization to reduce memory requirements\n",
    "2. **LoRA Configuration**: We apply adapters only to specific layers to make training efficient\n",
    "3. **Gradient Checkpointing**: Trades computation for memory to handle larger batch sizes\n",
    "4. **Remote Training**: The `@remote` decorator sends the job to a SageMaker training instance\n",
    "\n",
    "### Training Parameters:\n",
    "- `lora_r`: Rank of the low-rank update matrices (higher = more capacity but more parameters)\n",
    "- `lora_alpha`: Scaling factor for the LoRA updates \n",
    "- `gradient_accumulation_steps`: Number of forward/backward passes before updating weights\n",
    "- `learning_rate`: Controls how quickly the model adapts to the reward task\n",
    "\n",
    "The reward model will learn to predict which responses humans would prefer, providing the \"feedback\" signal for our PPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c62175-24e3-4e59-8ee0-1a0f3ca70418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "    \n",
    "def find_all_linear_names(hf_model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in hf_model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)   \n",
    "\n",
    "# Start training with remote decorator (https://docs.aws.amazon.com/sagemaker/latest/dg/train-remote-decorator.html). Additional job config is being pulled in from config.yaml. \n",
    "@remote(volume_size=100, job_name_prefix=f\"train-{model_id.split('/')[-1].replace('.', '-')}-reward\", use_torchrun=True, nproc_per_node=1, include_local_workdir=True,\n",
    "       #keep_alive_period_in_seconds=3600\n",
    "        )\n",
    "def train_fn(\n",
    "        model_name,\n",
    "        train_ds,\n",
    "        lora_r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=1,\n",
    "        fsdp=\"\",\n",
    "        fsdp_config=None,\n",
    "        chunk_size=10000,\n",
    "        gradient_checkpointing=False,\n",
    "        merge_weights=False,\n",
    "        seed=42,\n",
    "        token=None,\n",
    "        model_hub_repo_id=None,\n",
    "        range_train=None,\n",
    "        range_eval=None\n",
    "):\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Initialize Accelerator object handling distributed training\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    # Login to HuggingFace\n",
    "    if token is not None:\n",
    "        login(token)\n",
    "\n",
    "    # Load tokenizer. Padding side is \"left\" because focus needs to be on completion\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = \"left\")\n",
    "\n",
    "    # Set tokenizer's pad Token\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "\n",
    "    # Load data from S3\n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    dataset = load_from_disk(train_ds)  \n",
    "    \n",
    "    \n",
    "    # Allow for partial dataset training\n",
    "    if range_train:\n",
    "        train_dataset = dataset[\"train\"].select(range(range_train))\n",
    "    else: \n",
    "        train_dataset = dataset[\"train\"]\n",
    "  \n",
    "    if range_eval:\n",
    "        eval_dataset = dataset[\"test\"].select(range(range_eval))\n",
    "    else:\n",
    "        eval_dataset = dataset[\"test\"]\n",
    "\n",
    "    # Specify quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        quant_storage_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load model with classification head for reward\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        #num_labels=1,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "        #attn_implementation=\"flash_attention_2\",\n",
    "        use_cache=False if gradient_checkpointing else True,\n",
    "        cache_dir=\"/tmp/.cache\"\n",
    "    )\n",
    "    \n",
    "    # Pre-LoRA trainable paremeters\n",
    "    print_trainable_parameters(model)     \n",
    "    \n",
    "    # Set model pad token id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    # Prepare model for quantized training\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=gradient_checkpointing)\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Get lora target modules\n",
    "    modules = find_all_linear_names(model)\n",
    "    print(f\"Found {len(modules)} modules to quantize: {modules}\")\n",
    "    \n",
    "    # Specify LoRA config\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    \n",
    "    # Make sure to not train for CLM\n",
    "    if config.task_type != \"SEQ_CLS\":\n",
    "        warnings.warn(\n",
    "            \"You are using a `task_type` that is different than `SEQ_CLS` for PEFT. This will lead to silent bugs\"\n",
    "            \" Make sure to pass --lora_task_type SEQ_CLS when using this script.\"\n",
    "        )\n",
    "    \n",
    "    # Create PeftModel\n",
    "    model = get_peft_model(model, config)\n",
    "    \n",
    "    # Post-LoRA trainable paremeters\n",
    "    print_trainable_parameters(model)     \n",
    "    \n",
    "    # Specify training config\n",
    "    reward_config = RewardConfig(\n",
    "                        per_device_train_batch_size=per_device_train_batch_size,\n",
    "                        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "                        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                        gradient_checkpointing=gradient_checkpointing,\n",
    "                        logging_strategy=\"steps\",\n",
    "                        logging_steps=100,\n",
    "                        log_on_each_node=False,\n",
    "                        num_train_epochs=num_train_epochs,\n",
    "                        learning_rate=learning_rate,\n",
    "                        bf16=True,\n",
    "                        ddp_find_unused_parameters=False,\n",
    "                        fsdp=fsdp,\n",
    "                        fsdp_config=fsdp_config,\n",
    "                        save_strategy=\"no\",\n",
    "                        output_dir=\"outputs\",\n",
    "                        max_length=512, \n",
    "                        remove_unused_columns=False,\n",
    "                        gradient_checkpointing_kwargs = {\"use_reentrant\": False}\n",
    "                        )\n",
    "    \n",
    "    # Initialize RewardTrainer object handling training\n",
    "    trainer = RewardTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=reward_config,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    \n",
    "    trainer.model.save_pretrained(\"/opt/ml/model\", safe_serialization=True)\n",
    "    \n",
    "    if model_hub_repo_id is not None:\n",
    "        trainer.model.push_to_hub(repo_id=model_hub_repo_id)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        tokenizer.save_pretrained(\"/opt/ml/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c89257",
   "metadata": {},
   "source": [
    "## Step 7.2: Setting Up the Reward Model Repository\n",
    "\n",
    "Before launching the training job, we need to set up a repository on Hugging Face Hub to store our trained reward model adapter. This repository will:\n",
    "\n",
    "1. Serve as a permanent storage location for our trained adapter weights\n",
    "2. Make the reward model easily accessible for the subsequent PPO training phase\n",
    "3. Allow for version control and sharing of the model\n",
    "\n",
    "You can name the model repository `***HF_ALIAS***/llama-32-hhrlhf-reward-adapter`, where `HF_ALIAS`is your HuggingFace username. In case you haven't setup a model repository yet, please check the [HuggingFace documentation](https://huggingface.co/docs/hub/en/repositories-getting-started). \n",
    "\n",
    "<img src=\"./images/CreateHFRepo.png\" width=\"30%\" alt='CreateHFRepo.png'/> \n",
    "\n",
    "In a production environment, you might want to create a dedicated organization for your models rather than using a personal account. Make sure your Hugging Face token has write access to create and update repositories.\n",
    "\n",
    "Note: Please make sure to repace placeholder below with your `HF_ALIAS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a166e-9215-473d-b0c9-ce04ba8afc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your HF alias\n",
    "rm_adapter_hub_repo_id = \"***HF_ALIAS***/llama-32-hhrlhf-reward-adapter\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948018c",
   "metadata": {},
   "source": [
    "## Step 7.3: Launching Reward Model Training\n",
    "\n",
    "Now we're ready to start the reward model training job. This cell launches a SageMaker training instance via the `remote` decorator. \n",
    "\n",
    "### Understanding Key Parameters:\n",
    "- `model_id`: The base model we're fine-tuning (e.g. `meta-llama/Llama-3.2-1B-Instruct`)\n",
    "- `dataset_path_hhrlhf`: S3 path to our processed human preference dataset\n",
    "- `per_device_*_batch_size`: How many examples to process at once on each GPU\n",
    "- `gradient_accumulation_steps`: Accumulates gradients before weight updates to simulate larger batch sizes\n",
    "- `gradient_checkpointing`: Memory optimization technique (trades compute for memory)\n",
    "- `range_train/range_eval`: For workshop purposes, we're limiting the dataset size to complete training faster\n",
    "\n",
    "For a production model, you would want to:\n",
    "1. Use the full dataset instead of a small range\n",
    "2. Train for multiple epochs \n",
    "3. Optimize hyperparameters like learning rate and batch size\n",
    "\n",
    "When you run this cell, the training job will be dispatched to a remote instance. This process will take some time to complete, from 10-30 minutes depending on instance type and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603eefb2-0060-4d19-8f3d-cd340335eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training job\n",
    "train_fn(\n",
    "    model_id,\n",
    "    train_ds=dataset_path_hhrlhf,  # Use S3 path instead of in-memory dataset\n",
    "    #train_ds=tokenized_dataset_hhrlhf,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=1,\n",
    "    token=hf_token,\n",
    "    model_hub_repo_id=rm_adapter_hub_repo_id,\n",
    "    range_train=100,\n",
    "    range_eval=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2638a",
   "metadata": {},
   "source": [
    "## Step 8: Preparing for PPO Training\n",
    "\n",
    "Now that we've trained our reward model, we're ready to move on to the Proximal Policy Optimization (PPO) phase of RLHF. This is where the model learns to generate responses that maximize the reward predicted by our reward model.\n",
    "\n",
    "### Understanding Multi-Adapter PPO\n",
    "\n",
    "In our implementation, we're using a multi-adapter approach for PPO. This means:\n",
    "\n",
    "1. **One Base Model**: The foundation model (Meta Llama 3.1 8B Instruct / Meta Llama 3.2 1B Instruct)\n",
    "2. **Two Adapter Sets**:\n",
    "   - **Policy Adapter**: The model being optimized through RL\n",
    "   - **Reward Adapter**: Our trained model that evaluates response quality\n",
    "\n",
    "This approach is efficient because:\n",
    "- We only need to load one copy of the large base model in memory\n",
    "- Multiple adapters can be attached to the same base model\n",
    "- Each adapter only adds a small number of trainable parameters\n",
    "\n",
    "<img src=\"./images/MultiAdapterPPO.png\" width=\"30%\" alt='MultiAdapterPPO.png'/> \n",
    "\n",
    "The following function implements the PPO training loop with reward computation all within a single model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33d9f0-b84d-44e9-9920-5f94f19e7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set path to config file for remote decorator\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e2e96-02cc-44bd-a389-f490eb5ae94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "# Start training with remote decorator (https://docs.aws.amazon.com/sagemaker/latest/dg/train-remote-decorator.html). Additional job config is being pulled in from config.yaml. \n",
    "@remote(volume_size=100, job_name_prefix=f\"train-{model_id.split('/')[-1].replace('.', '-')}-multi-adapter-ppo\", use_torchrun=True, nproc_per_node=1\n",
    "       #keep_alive_period_in_seconds=3600\n",
    "       )\n",
    "def train_fn(\n",
    "        model_name,\n",
    "        train_ds,\n",
    "        rm_adapter,\n",
    "        s3_output_path,\n",
    "        log_with=None,\n",
    "        use_safetensors=None,\n",
    "        use_score_scaling=False,\n",
    "        use_score_norm=False,\n",
    "        score_clip=None,\n",
    "        seed=42,\n",
    "        token=None,\n",
    "        model_hub_repo_id=None,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        gradient_checkpointing=True,\n",
    "        num_train_epochs=1,\n",
    "        merge_weights=True,\n",
    "        range_train=None,\n",
    "        ):\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Initialize Accelerator object handling distributed training\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Login to HuggingFace \n",
    "    if token is not None:\n",
    "        login(token)\n",
    "        \n",
    "    # Load tokenizer. Padding side is \"left\" because focus needs to be on completion\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "    # Set tokenizer's pad Token\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id  \n",
    "    \n",
    "    \n",
    "    # Load data from S3\n",
    "    dataset = load_from_disk(train_ds)\n",
    "    \n",
    "    \n",
    "    # Allow for partial dataset training\n",
    "    if range_train:\n",
    "        train_dataset = dataset[\"train\"].select(range(range_train))\n",
    "    else: \n",
    "        train_dataset = dataset[\"train\"]\n",
    "    \n",
    "    # Specify LoRA config\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    # Specify quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        model_name,\n",
    "        #device_map='auto',\n",
    "        peft_config=lora_config,\n",
    "        quantization_config=bnb_config,\n",
    "        reward_adapter=rm_adapter,\n",
    "        use_safetensors=use_safetensors,\n",
    "        #attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    \n",
    "    # Set model pad token id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "        \n",
    "    # Trainable paremeters\n",
    "    print_trainable_parameters(model)    \n",
    "\n",
    "    def collator(data):\n",
    "        return {key: [d[key] for d in data] for key in data[0]}\n",
    "\n",
    "    # Specify PPO training config\n",
    "    config = PPOConfig(\n",
    "        model_name,\n",
    "        log_with=None,\n",
    "        learning_rate=1e-5,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        mini_batch_size=1,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optimize_cuda_cache=True,\n",
    "        seed=42,\n",
    "        use_score_scaling=False,\n",
    "        use_score_norm=False,\n",
    "        score_clip=None,\n",
    "    )\n",
    "\n",
    "    # Initialize PPOTrainer object handling training\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config,\n",
    "        model,\n",
    "        ref_model=None,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=train_dataset,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    # Specifying inference params\n",
    "    generation_kwargs = {\n",
    "        #\"top_k\": 0.0,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"max_new_tokens\": 32,\n",
    "    }\n",
    "    \n",
    "    step = 0\n",
    "\n",
    "    for _epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "        \n",
    "        question_tensors = batch[\"input_ids\"]\n",
    "        \n",
    "        # Inference through model being fine-tuned\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            question_tensors,\n",
    "            return_prompt=False,\n",
    "            **generation_kwargs,\n",
    "        )\n",
    "        \n",
    "        # Decode response\n",
    "        batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "        \n",
    "        # Concat query and response\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        \n",
    "        # Tokenize query - response pair\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(ppo_trainer.accelerator.device)\n",
    "        \n",
    "        # Compute reward score\n",
    "        raw_rewards = ppo_trainer.accelerator.unwrap_model(ppo_trainer.model).compute_reward_score(**inputs)\n",
    "        rewards = [raw_rewards[i, -1, 1] for i in range(len(raw_rewards))]  # take last token\n",
    "\n",
    "        # Run PPO step\n",
    "        stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "        \n",
    "        step = step + 1      \n",
    "\n",
    "\n",
    "    if merge_weights:\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            \n",
    "            output_dir = \"/tmp/model\"\n",
    "    \n",
    "\n",
    "            ppo_trainer.save_pretrained(output_dir, safe_serialization=True)\n",
    "\n",
    "       \n",
    "            # clear memory\n",
    "            del model\n",
    "            del ppo_trainer\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # load PEFT model\n",
    "            model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "                output_dir,\n",
    "                torch_dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                use_cache=True,\n",
    "                cache_dir=\"/tmp/.cache\",\n",
    "            )\n",
    "\n",
    "            # Merge LoRA and base model and save\n",
    "            model = model.merge_and_unload()\n",
    "            model.save_pretrained(\n",
    "                os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\"),\n",
    "                safe_serialization=True,\n",
    "                max_shard_size=\"2GB\"\n",
    "            )\n",
    "            if model_hub_repo_id is not None:\n",
    "                model.push_to_hub(repo_id=model_hub_repo_id)\n",
    "\n",
    "            tokenizer.save_pretrained(os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\"))\n",
    "\n",
    "            if model_hub_repo_id is not None:\n",
    "                tokenizer.push_to_hub(repo_id=model_hub_repo_id)\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    else:\n",
    "        if accelerator.is_main_process:\n",
    "            \n",
    "            ppo_trainer.model.module.save_pretrained(\n",
    "                os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\"),\n",
    "                safe_serialization=True\n",
    "            )\n",
    "    \n",
    "            if model_hub_repo_id is not None:\n",
    "                ppo_trainer.push_to_hub(repo_id=model_hub_repo_id)\n",
    "    \n",
    "    \n",
    "            tokenizer.save_pretrained(os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\"))\n",
    "    \n",
    "            if model_hub_repo_id is not None:\n",
    "                tokenizer.push_to_hub(repo_id=model_hub_repo_id)\n",
    "\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        # Upload the model files to S3\n",
    "       \n",
    "        \n",
    "        # Get the S3 output path from the environment variables\n",
    "        # SageMaker automatically sets these environment variables\n",
    "        if os.environ.get(\"SM_MODEL_DIR\") and s3_output_path:\n",
    "            model_dir = os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\")\n",
    "            \n",
    "            print(f\"Uploading model from {model_dir} to {s3_output_path}\")\n",
    "            \n",
    "            # Initialize S3 client\n",
    "            s3_client = boto3.client('s3')\n",
    "            \n",
    "            # Extract bucket name and prefix from S3 URI\n",
    "            s3_uri_parts = s3_output_path.replace(\"s3://\", \"\").split(\"/\")\n",
    "            bucket_name = s3_uri_parts[0]\n",
    "            prefix = \"/\".join(s3_uri_parts[1:]) if len(s3_uri_parts) > 1 else \"\"\n",
    "            \n",
    "            # Walk through all files in the model directory and upload them\n",
    "            for root, dirs, files in os.walk(model_dir):\n",
    "                for file in files:\n",
    "                    local_path = os.path.join(root, file)\n",
    "                    # Create relative path to maintain directory structure\n",
    "                    relative_path = os.path.relpath(local_path, model_dir)\n",
    "                    s3_key = os.path.join(prefix, relative_path)\n",
    "                    \n",
    "                    print(f\"Uploading {local_path} to s3://{bucket_name}/{s3_key}\")\n",
    "                    try:\n",
    "                        s3_client.upload_file(local_path, bucket_name, s3_key)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to upload {local_path} to S3: {e}\")\n",
    "            \n",
    "            print(\"Model upload to S3 completed\")\n",
    "\n",
    "    # Wait for all processes to complete\n",
    "    accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dee2b8",
   "metadata": {},
   "source": [
    "## Step 8.1: Creating the HuggingFace Repository for the RLHF Policy Model\n",
    "\n",
    "Similar to what we did for the reward model, we need to set up a repository to store our final RLHF-trained policy model. This time, with `***HF_ALIAS***/llama-32-hhrlhf-squad-rlhf-policy-model` we're choosing a name that indicates this is the full policy model rather than just an adapter.\n",
    "\n",
    "For a production workflow, consider:\n",
    "1. Using descriptive naming conventions that indicate model version and training approach\n",
    "2. Setting up model cards on HuggingFace that document the model's capabilities and limitations\n",
    "3. Implementing proper access control for sensitive or proprietary models\n",
    "\n",
    "Note: Please make sure to repace placeholder below with your `HF_ALIAS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941a1b3-8bbe-44df-b18f-4c6fb5e82de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hub_repo_id = \"***HF_ALIAS***/llama-31-hhrlhf-squad-rlhf-policy-model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ca735",
   "metadata": {},
   "source": [
    "## Step 8.2: Setting Up the Hugging Face Authentication\n",
    "\n",
    "Similar to the reward model training step, we need to provide our Hugging Face token to authenticate with the Hub. This token will be used to push our trained model to the repository we specified above.\n",
    "\n",
    "> Note: For a real workshop, don't show your token directly in the notebook. Consider using environment variables or a secrets manager for sensitive credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf52e40-0e75-404d-aa3e-c44810a09fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8cd368",
   "metadata": {},
   "source": [
    "## Step 8.3: Launching the PPO Training Job\n",
    "\n",
    "Now we'll launch the PPO training job to fine-tune our model using the reward model we trained earlier. This step is where the actual reinforcement learning happens.\n",
    "\n",
    "### PPO Training Process:\n",
    "1. The model generates responses to prompts from the SQuAD dataset\n",
    "2. Each response is scored by the reward model\n",
    "3. The PPO algorithm uses these scores to update the policy model\n",
    "4. This process iterates to maximize the expected reward\n",
    "\n",
    "### Key Parameters:\n",
    "- `model_id`: The base model we're fine-tuning\n",
    "- `train_ds`: The dataset of prompts for PPO training\n",
    "- `rm_adapter`: The path to our trained reward model\n",
    "- `merge_weights`: If True, merges the adapters with the base model for deployment\n",
    "- `range_train`: Number of examples to use for training (limited for workshop)\n",
    "\n",
    "This training job will take 15-45 minutes depending on the compute instance and dataset size. For a production model, you would want to use the full dataset and tune hyperparameters carefully.\n",
    "\n",
    "While training, the model gradually learns to generate responses that the reward model will score highly, effectively aligning with human preferences encoded in the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782dc1a-67cd-4b3a-9ae0-16dfa3e57775",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn(\n",
    "    model_id,\n",
    "    train_ds=dataset_path_squad,  # Use S3 path instead of in-memory dataset\n",
    "    #train_ds=tokenized_dataset_squad,\n",
    "    s3_output_path=s3_output_path,\n",
    "    rm_adapter=rm_adapter_hub_repo_id,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=1,\n",
    "    token=hf_token,\n",
    "    model_hub_repo_id=model_hub_repo_id,\n",
    "    range_train=50,\n",
    "    merge_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fda06d",
   "metadata": {},
   "source": [
    "## Step 9: Deploying the Model to Amazon Bedrock\n",
    "\n",
    "After training our RLHF policy model, we want to make it available for inference. Amazon Bedrock provides a serverless endpoint for model deployment, allowing us to serve the model without managing infrastructure.\n",
    "\n",
    "The Custom Model Import (CMI) feature of Amazon Bedrock lets us import our fine-tuned model and access it through the same API as other foundation models. This provides:\n",
    "\n",
    "1. Scalable, serverless infrastructure\n",
    "2. Built-in security and compliance features\n",
    "3. Easy integration with other AWS services\n",
    "4. Cost-effective pay-per-use pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d5c356",
   "metadata": {},
   "source": [
    "## Step 9.1: Creating the Bedrock Model Import Job\n",
    "\n",
    "First, we need to set up an IAM role that gives Bedrock permission to access our model files in S3. Once we've set up the necessary permissions, we can create a model import job in Amazon Bedrock. This process:\n",
    "\n",
    "1. Takes our fine-tuned model from S3\n",
    "2. Optimizes it for the Bedrock infrastructure\n",
    "3. Makes it available through the Bedrock API\n",
    "\n",
    "We'll specify:\n",
    "- A unique name for our imported model\n",
    "- The IAM role we created for Bedrock\n",
    "- The S3 path where our trained model is stored\n",
    "\n",
    "The import process will take some time (typically ~5 minutes) as Bedrock prepares your model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad3ffbf-451c-43bd-a39a-1bbaa666fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize IAM client\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "# Role name - consider making this unique if you create multiple roles\n",
    "role_name = \"BedrockCustomModelImportRole\"\n",
    "\n",
    "# Define trust policy to allow Bedrock to assume this role\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define permissions policy for Bedrock model import\n",
    "bedrock_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::*/*\",\n",
    "                \"arn:aws:s3:::*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateModelCustomizationJob\",\n",
    "                \"bedrock:CreateModelImportJob\",\n",
    "                \"bedrock:GetModelCustomizationJob\",\n",
    "                \"bedrock:GetModelImportJob\",\n",
    "                \"bedrock:StopModelCustomizationJob\",\n",
    "                \"bedrock:StopModelImportJob\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"kms:Decrypt\",\n",
    "                \"kms:GenerateDataKey\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"cloudwatch:PutMetricData\"\n",
    "            ],\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"cloudwatch:namespace\": \"AWS/Bedrock\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/bedrock/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create or update the role\n",
    "try:\n",
    "    # Check if role exists\n",
    "    try:\n",
    "        iam.get_role(RoleName=role_name)\n",
    "        print(f\"Role {role_name} already exists. Updating policies...\")\n",
    "        # Delete any existing policies to ensure clean slate\n",
    "        for policy in iam.list_attached_role_policies(RoleName=role_name)['AttachedPolicies']:\n",
    "            iam.detach_role_policy(RoleName=role_name, PolicyArn=policy['PolicyArn'])\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'NoSuchEntity':\n",
    "            # Create role with trust policy if it doesn't exist\n",
    "            print(f\"Creating new role: {role_name}\")\n",
    "            iam.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "                Description=\"Role for Amazon Bedrock Model Import operations\"\n",
    "            )\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Create policy for Bedrock operations\n",
    "    policy_name = f\"{role_name}Policy\"\n",
    "    \n",
    "    # Check if policy exists and delete if it does\n",
    "    try:\n",
    "        existing_policy = iam.get_policy(PolicyArn=f\"arn:aws:iam::{boto3.client('sts').get_caller_identity()['Account']}:policy/{policy_name}\")\n",
    "        \n",
    "        # Detach policy if attached to our role\n",
    "        try:\n",
    "            iam.detach_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyArn=existing_policy['Policy']['Arn']\n",
    "            )\n",
    "        except ClientError:\n",
    "            pass  # Policy may not be attached\n",
    "        \n",
    "        # Delete existing versions (except default)\n",
    "        policy_versions = iam.list_policy_versions(PolicyArn=existing_policy['Policy']['Arn'])['Versions']\n",
    "        for version in policy_versions:\n",
    "            if not version['IsDefaultVersion']:\n",
    "                iam.delete_policy_version(\n",
    "                    PolicyArn=existing_policy['Policy']['Arn'],\n",
    "                    VersionId=version['VersionId']\n",
    "                )\n",
    "        \n",
    "        # Create new version and set as default\n",
    "        iam.create_policy_version(\n",
    "            PolicyArn=existing_policy['Policy']['Arn'],\n",
    "            PolicyDocument=json.dumps(bedrock_policy_document),\n",
    "            SetAsDefault=True\n",
    "        )\n",
    "        policy_arn = existing_policy['Policy']['Arn']\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'NoSuchEntity':\n",
    "            # Create new policy\n",
    "            response = iam.create_policy(\n",
    "                PolicyName=policy_name,\n",
    "                PolicyDocument=json.dumps(bedrock_policy_document),\n",
    "                Description=\"Permissions for Amazon Bedrock Model Import operations\"\n",
    "            )\n",
    "            policy_arn = response['Policy']['Arn']\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Attach policy to role\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=policy_arn\n",
    "    )\n",
    "    \n",
    "    # Wait for policy to propagate\n",
    "    print(f\"Waiting for IAM role and policies to propagate...\")\n",
    "    time.sleep(10)  # IAM changes can take a few seconds to propagate\n",
    "    \n",
    "    # Get the role ARN\n",
    "    role = iam.get_role(RoleName=role_name)\n",
    "    role_arn = role['Role']['Arn']\n",
    "    \n",
    "    print(f\"Successfully configured role: {role_arn}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up IAM role: {str(e)}\")\n",
    "    role_arn = None  # Set to None if there was an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f88673-d1ce-4b53-a93e-da620e68c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client('bedrock', region_name=Session().boto_region_name)\n",
    "\n",
    "# Define name for imported model\n",
    "imported_model_name = f'llama-31-hhrlhf-squad-rlhf-policy-model-{int(time.time())}'\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=imported_model_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=role_arn,\n",
    "    modelDataSource={\n",
    "        's3DataSource': {\n",
    "            's3Uri': s3_output_path\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "job_Arn = response['jobArn']\n",
    "\n",
    "# Output the job ARN\n",
    "print(f\"Model import job created with ARN: {response['jobArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be88ae",
   "metadata": {},
   "source": [
    "## Step 9.2: Monitoring the Import Job\n",
    "\n",
    "After initiating the model import, we need to monitor its progress. The import job goes through several states:\n",
    "- PENDING: Job is queued\n",
    "- IN_PROGRESS: Model is being imported\n",
    "- COMPLETED: Import successful\n",
    "- FAILED: Import encountered errors\n",
    "\n",
    "This cell polls the Bedrock API every 60 seconds to check the status of our import job, continuing until it reaches a terminal state (COMPLETED or FAILED). Once the job completes successfully, we'll have the model ARN which we can use for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bb7e6-d979-45a0-af06-05fbee5c22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CMI job status\n",
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_Arn)\n",
    "    status = response['status'].upper()\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)  # Check every 60 seconds\n",
    "\n",
    "# Get the model ID\n",
    "model_arn = response['importedModelArn']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4b47de",
   "metadata": {},
   "source": [
    "## Step 10: Testing the Deployed Model\n",
    "\n",
    "Now that our RLHF-fine-tuned model is deployed to Amazon Bedrock, we can invoke it for inference. We'll set up the necessary clients and functions to interact with our model through the Bedrock Runtime API.\n",
    "\n",
    "### Inference Setup Components:\n",
    "1. **Tokenizer**: To properly format inputs for the model\n",
    "2. **Bedrock Runtime Client**: AWS SDK client for making inference calls\n",
    "3. **Helper Function**: To handle retry logic and properly format requests\n",
    "\n",
    "The `generate` function we're defining:\n",
    "- Applies the proper chat template to user messages\n",
    "- Handles retry logic for robustness\n",
    "- Sets appropriate generation parameters like temperature and top-p\n",
    "\n",
    "This setup allows us to easily test how well our RLHF training worked by sending queries to the model and evaluating its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e945ed2-fbce-4658-903b-d7c35b6a5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2',\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,     # 5 minutes\n",
    "        retries={'max_attempts': 3}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386e6b2-c842-4f8f-bb1f-beeee1bb1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(messages, temperature=0.3, max_tokens=4096, top_p=0.9, continuation=False, max_retries=10):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "    \n",
    "    Parameters:\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        continuation (bool): Whether this is a continuation of previous generation\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, \n",
    "                                         add_generation_prompt=not continuation)\n",
    "    \n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            response = client.invoke_model(\n",
    "                modelId=model_arn,\n",
    "                body=json.dumps({\n",
    "                    'prompt': prompt,\n",
    "                    'temperature': temperature,\n",
    "                    'max_gen_len': max_tokens,\n",
    "                    'top_p': top_p\n",
    "                }),\n",
    "                accept='application/json',\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response['body'].read().decode('utf-8'))\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            attempt += 1\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(30)\n",
    "    \n",
    "    raise Exception(\"Failed to get response after maximum retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc399ad",
   "metadata": {},
   "source": [
    "## Step 10.1: Testing Our RLHF-Trained Model\n",
    "\n",
    "Now let's put our model to the test with a simple prompt. This will help us evaluate whether the RLHF fine-tuning has improved the model's responses in alignment with human preferences.\n",
    "\n",
    "When testing an RLHF model, consider the following evaluation criteria:\n",
    "\n",
    "1. **Helpfulness**: Does the model provide useful information that addresses the user's query?\n",
    "2. **Harmlessness**: Does the model avoid generating harmful, misleading, or inappropriate content?\n",
    "3. **Alignment**: Does the response generally match what humans would prefer?\n",
    "4. **Factuality**: Is the information accurate and well-supported?\n",
    "5. **Style**: Is the response well-structured and appropriately formatted?\n",
    "\n",
    "For a comprehensive evaluation, you would want to:\n",
    "- Compare responses with the original base model\n",
    "- Test a diverse set of prompts, including edge cases\n",
    "- Gather human feedback on the responses\n",
    "- Use quantitative metrics like ROUGE or BERTScore\n",
    "\n",
    "In this workshop, we'll do a simple test with a conversational greeting to verify the model is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8889a-f065-42e3-a2be-eb459f30596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"Hi, how are you?\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "response = generate(messages)\n",
    "print(\"Model Response:\")\n",
    "print(response[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27e1ef",
   "metadata": {},
   "source": [
    "## Step 11: Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "1. Prepared datasets for both reward model and PPO training\n",
    "2. Trained a reward model that can evaluate response quality based on human preferences\n",
    "3. Fine-tuned an LLM using RLHF with multi-adapter PPO\n",
    "4. Deployed the model to Amazon Bedrock for serverless inference\n",
    "\n",
    "### Key Takeaways:\n",
    "- RLHF is a powerful technique for aligning LLMs with human preferences\n",
    "- The multi-adapter approach allows for efficient training with limited resources\n",
    "- Parameter-efficient techniques like LoRA adapters make fine-tuning accessible\n",
    "- Serverless deployment enables practical use without complex infrastructure\n",
    "\n",
    "Remember that RLHF is an iterative process, and improving alignment often requires multiple rounds of training with increasingly refined preference data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b95faca",
   "metadata": {},
   "source": [
    "## Step 12: Resource Cleanup\n",
    "\n",
    "To avoid incurring unnecessary costs after completing this workshop, it's important to clean up all the resources we've created. This includes removing:\n",
    "\n",
    "1. The imported Bedrock model\n",
    "2. The S3 bucket and its contents\n",
    "3. IAM roles and policies created for Bedrock\n",
    "4. Local downloaded files\n",
    "\n",
    "Following proper cleanup practices ensures you don't have unexpected charges on your AWS account and maintains good security hygiene by removing permissions that are no longer needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f05a2",
   "metadata": {},
   "source": [
    "## Workshop Summary\n",
    "\n",
    "You've completed the RLHF Workshop focused on fine-tuning LLMs using multi-adapter PPO. Throughout this tutorial, you've gained practical experience with:\n",
    "\n",
    "1. **Data Preparation**: Processing human preference data for reward modeling and creating prompt datasets for PPO training\n",
    "\n",
    "2. **Reward Modeling**: Training a model to distinguish between preferred and non-preferred responses based on human feedback\n",
    "\n",
    "3. **RLHF Training**: Using PPO to optimize a language model policy to generate responses that align with human preferences\n",
    "\n",
    "4. **Efficient Fine-tuning**: Leveraging parameter-efficient techniques like LoRA adapters to make training more accessible and cost-effective\n",
    "\n",
    "5. **Model Deployment**: Creating serverless inference endpoints on Amazon Bedrock for your custom fine-tuned model\n",
    "\n",
    "These skills form the foundation of modern LLM alignment techniques used by leading AI labs and companies to create helpful, harmless, and honest AI assistants. By continuing to experiment with different datasets, model sizes, and training configurations, you'll develop deeper expertise in RLHF and contribute to advancing responsible AI development.\n",
    "\n",
    "Thank you for participating in this workshop!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
